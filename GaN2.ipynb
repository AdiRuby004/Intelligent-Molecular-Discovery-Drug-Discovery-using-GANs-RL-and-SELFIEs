{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4Mymlud5ZwH",
        "outputId": "16335446-1e4b-498d-bf6d-d79dcfae6084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting pypi\n",
            "  Downloading pypi-2.1.tar.gz (997 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting selfies\n",
            "  Downloading selfies-2.2.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading rdkit-2025.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (36.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selfies-2.2.0-py3-none-any.whl (36 kB)\n",
            "Building wheels for collected packages: pypi\n",
            "  Building wheel for pypi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypi: filename=pypi-2.1-py3-none-any.whl size=1334 sha256=4e5b0a170d1688b2c11ba23ac8d05b68d85139acc0883cbd94a838996cedabd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/4c/49/00cdce1e7a68a48810e9203391f80f4c7344a5e4ad9d4d6649\n",
            "Successfully built pypi\n",
            "Installing collected packages: pypi, selfies, rdkit\n",
            "Successfully installed pypi-2.1 rdkit-2025.9.1 selfies-2.2.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install torch rdkit pypi selfies pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG1xFlCq5Uz3",
        "outputId": "c7a8a8f0-c310-48d1-e628-8484639ecd2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import selfies as sf\n",
        "import random\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import QED, Crippen, Descriptors\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6becc4fc0f244881887d3a6de1cc3cd6",
            "d50bc005c7d2401a949f7dfc504986b6",
            "98951c648f9d40838666155b358738a1",
            "308d5df0f6c5406b9409350f4f21a647",
            "32274995ef8f413296d9be0a1c3da1b0",
            "fd59e1082a034203857a02615e23f6e5",
            "6e12a98dd24d448cbbc47ffb919fdd21",
            "18546673187c4cad9470f0f73436557f",
            "6b946c4ec47942ce88ac9e6676595aa3",
            "47e47b9ee2114467b16a5a3598549503",
            "4b48b49b49204787aee6aeeb41e2a0c4"
          ]
        },
        "id": "ucmEenaY5fjZ",
        "outputId": "0ade5146-228f-48ea-8625-8e6b74213ffb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6becc4fc0f244881887d3a6de1cc3cd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Encoding SELFIES:   0%|          | 0/1936962 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 29\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Download and load the dataset\n",
        "url = \"https://media.githubusercontent.com/media/molecularsets/moses/master/data/dataset_v1.csv\"\n",
        "df = pd.read_csv(url)\n",
        "smiles_list = df['SMILES'].tolist()\n",
        "\n",
        "# Convert SMILES to SELFIES [cite: 65]\n",
        "selfies_list = [sf.encoder(smi) for smi in tqdm(smiles_list, desc=\"Encoding SELFIES\")]\n",
        "\n",
        "# Cell 4: Build Vocabulary\n",
        "def build_vocab(selfies_list):\n",
        "    \"\"\"Builds a vocabulary and token mappings from a list of SELFIES strings.\"\"\"\n",
        "    tokens = set()\n",
        "    for s in selfies_list:\n",
        "        tokens.update(sf.split_selfies(s))\n",
        "\n",
        "    vocab = sorted(list(tokens))\n",
        "    vocab.insert(0, '<pad>') # Padding token\n",
        "    vocab.insert(1, '<sos>') # Start of sequence\n",
        "    vocab.insert(2, '<eos>') # End of sequence\n",
        "\n",
        "    token_to_int = {token: i for i, token in enumerate(vocab)}\n",
        "    int_to_token = {i: token for i, token in enumerate(vocab)}\n",
        "\n",
        "    return vocab, token_to_int, int_to_token\n",
        "\n",
        "vocab, token_to_int, int_to_token = build_vocab(selfies_list)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Cell 5: Create a PyTorch Dataset\n",
        "class SELFIESDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for SELFIES sequences.\"\"\"\n",
        "    def __init__(self, selfies_list, token_to_int, max_len=128):\n",
        "        self.selfies_list = selfies_list\n",
        "        self.token_to_int = token_to_int\n",
        "        self.max_len = max_len\n",
        "        self.pad_token_id = token_to_int['<pad>']\n",
        "        self.sos_token_id = token_to_int['<sos>']\n",
        "        self.eos_token_id = token_to_int['<eos>']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.selfies_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        selfies = sf.split_selfies(self.selfies_list[idx])\n",
        "        tokens = ['<sos>'] + list(selfies) + ['<eos>']\n",
        "\n",
        "        # Truncate if longer than max_len\n",
        "        if len(tokens) > self.max_len:\n",
        "            tokens = tokens[:self.max_len]\n",
        "\n",
        "        # Convert to integers\n",
        "        token_ids = [self.token_to_int.get(token, self.pad_token_id) for token in tokens]\n",
        "\n",
        "        # Pad sequence\n",
        "        padding_len = self.max_len - len(token_ids)\n",
        "        token_ids += [self.pad_token_id] * padding_len\n",
        "\n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset = SELFIESDataset(selfies_list, token_to_int, max_len=MAX_LEN)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wxbScTgE5k9g"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Transformer Generator [cite: 66, 95]\n",
        "class TransformerGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=512, max_seq_length=128):\n",
        "        super(TransformerGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers,\n",
        "                                           num_decoder_layers, dim_feedforward, batch_first=True)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model) + self.positional_encoding[:, :src.size(1), :]\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) + self.positional_encoding[:, :tgt.size(1), :]\n",
        "\n",
        "        # Create masks\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
        "\n",
        "        output = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
        "        return self.fc_out(output)\n",
        "\n",
        "# Modified CNNDiscriminator to accept pre-embedded inputs\n",
        "class CNNDiscriminator(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, max_seq_length=128):\n",
        "        super(CNNDiscriminator, self).__init__()\n",
        "        # The embedding layer is still part of the model's parameters\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, 100, (k, d_model)) for k in [2, 3, 4, 5]\n",
        "        ])\n",
        "        self.fc = nn.Linear(400, 1)\n",
        "\n",
        "    def forward(self, embedded_x):\n",
        "        # The forward pass now starts with the already embedded input\n",
        "        # The input should have shape: (batch_size, seq_len, d_model)\n",
        "        embedded = embedded_x.unsqueeze(1) # (batch_size, 1, seq_len, d_model)\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in conved]\n",
        "        cat = torch.cat(pooled, 1)\n",
        "        return self.fc(cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "476072f0"
      },
      "outputs": [],
      "source": [
        "# Define Hyperparameters for GAN Training\n",
        "D_MODEL = 128     # Dimension of the model\n",
        "LR = 1e-4         # Learning rate\n",
        "LAMBDA_GP = 10.0  # Gradient penalty coefficient\n",
        "N_EPOCHS = 50      # Number of epochs for GAN training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "a4fde947dec54cdc8e8efc59efdf7100"
          ]
        },
        "id": "PMyeCfzy5l4n",
        "outputId": "f75359e0-185e-4ffa-cd6d-85f056e0dc19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ No WGAN checkpoint found. Starting training from scratch.\n",
            "Starting WGAN-GP Training...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4fde947dec54cdc8e8efc59efdf7100",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/50:   0%|          | 0/15625 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-676791586.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moptimizer_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mnoise_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mfake_output_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mfake_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mpad_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1157812849.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_is_causal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         )\n\u001b[0;32m--> 280\u001b[0;31m         output = self.decoder(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    629\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             x = self.norm1(\n\u001b[0;32m-> 1123\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m             )\n\u001b[1;32m   1125\u001b[0m             x = self.norm2(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0mis_causal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     ) -> Tensor:\n\u001b[0;32m-> 1143\u001b[0;31m         x = self.self_attn(\n\u001b[0m\u001b[1;32m   1144\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             )\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1381\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6302\u001b[0m             \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6303\u001b[0m         )\n\u001b[0;32m-> 6304\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6305\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6306\u001b[0m         assert q_proj_weight is not None, (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   5700\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5701\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5702\u001b[0;31m                 \u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5703\u001b[0m             )\n\u001b[1;32m   5704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Cell 8: WGAN-GP Gradient Penalty Calculation (Your improved version is kept)\n",
        "def compute_gradient_penalty(discriminator, real_samples_embedded, fake_samples_embedded):\n",
        "    \"\"\"Calculates the gradient penalty for WGAN-GP on embedded samples.\"\"\"\n",
        "    alpha = torch.randn(real_samples_embedded.size(0), 1, 1).to(device)\n",
        "    alpha = alpha.expand_as(real_samples_embedded)\n",
        "\n",
        "    interpolates = (alpha * real_samples_embedded + ((1 - alpha) * fake_samples_embedded)).requires_grad_(True)\n",
        "    d_interpolates = discriminator(interpolates)\n",
        "\n",
        "    fake = torch.autograd.Variable(torch.Tensor(real_samples_embedded.shape[0], 1).fill_(1.0), requires_grad=False).to(device)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Cell 9: Corrected Training Loop\n",
        "# (Re-initialize models and optimizers before running this cell)\n",
        "generator = TransformerGenerator(vocab_size, d_model=D_MODEL, max_seq_length=MAX_LEN).to(device)\n",
        "discriminator = CNNDiscriminator(vocab_size, d_model=D_MODEL, max_seq_length=MAX_LEN).to(device)\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.9))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.9))\n",
        "criterion_g = nn.CrossEntropyLoss(ignore_index=token_to_int['<pad>'])\n",
        "\n",
        "print(\"Starting Corrected WGAN-GP Training...\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "    for i, real_seq in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\")):\n",
        "        real_seq = real_seq.to(device)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # --- FIX: Generate from random noise to create NOVEL molecules ---\n",
        "        # The noise_input acts as the latent vector (source for generation)\n",
        "        noise_input = torch.randint(0, vocab_size, real_seq.shape, device=device)\n",
        "        # We still use teacher-forcing with the real sequence as the target\n",
        "        fake_output_logits = generator(noise_input, real_seq[:, :-1])\n",
        "        fake_seq = torch.argmax(fake_output_logits, dim=-1)\n",
        "\n",
        "        # --- FIX: Pad the fake sequence to match the real sequence length ---\n",
        "        pad_token_id = token_to_int['<pad>']\n",
        "        # Pad on the right side (last dimension) with one padding token\n",
        "        fake_seq_padded = F.pad(fake_seq, (0, 1), \"constant\", pad_token_id) # Shape: (batch_size, 128)\n",
        "\n",
        "        # Now embed the sequences of matching lengths\n",
        "        real_seq_embedded = discriminator.embedding(real_seq)\n",
        "        fake_seq_embedded = discriminator.embedding(fake_seq_padded.detach())\n",
        "\n",
        "\n",
        "        real_validity = discriminator(real_seq_embedded)\n",
        "        fake_validity = discriminator(fake_seq_embedded)\n",
        "\n",
        "        gradient_penalty = compute_gradient_penalty(discriminator, real_seq_embedded.detach(), fake_seq_embedded.detach())\n",
        "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + LAMBDA_GP * gradient_penalty\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        if i % 5 == 0:\n",
        "            optimizer_g.zero_grad()\n",
        "\n",
        "            # --- FIX: Ensure generator is trained to generate from noise ---\n",
        "            noise_input_g = torch.randint(0, vocab_size, real_seq.shape, device=device)\n",
        "            fake_output_logits_g = generator(noise_input_g, real_seq[:, :-1])\n",
        "            fake_seq_g = torch.argmax(fake_output_logits_g, dim=-1)\n",
        "\n",
        "            # --- KEPT IMPROVEMENT: Embed fake sequences for discriminator input ---\n",
        "            fake_seq_g_embedded = discriminator.embedding(fake_seq_g)\n",
        "            adv_loss = -torch.mean(discriminator(fake_seq_g_embedded))\n",
        "\n",
        "            # Supervised Loss (helps with stability)\n",
        "            sup_loss = criterion_g(fake_output_logits_g.view(-1, vocab_size), real_seq[:, 1:].reshape(-1))\n",
        "\n",
        "            g_loss = adv_loss + sup_loss\n",
        "            g_loss.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "    if 'd_loss' in locals():\n",
        "      print(f\"[Epoch {epoch+1}/{N_EPOCHS}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wXQHV_M5oYf"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Multi-Objective Reward Function [cite: 54, 101]\n",
        "def calculate_reward(selfies_string):\n",
        "    \"\"\"Calculates a weighted reward for a molecule based on QED, logP, and SA.\"\"\"\n",
        "    try:\n",
        "        smi = sf.decoder(selfies_string)\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None: return 0.0\n",
        "\n",
        "        qed = QED.qed(mol)\n",
        "        logp = Crippen.MolLogP(mol)\n",
        "\n",
        "        # A simple SA score-like penalty based on molecular weight\n",
        "        mw = Descriptors.MolWt(mol)\n",
        "        sa_penalty = max(0, (mw - 350) / 100) # Penalize high MW\n",
        "\n",
        "        # Define weights for each property\n",
        "        w1, w2, w3 = 0.5, 0.25, 0.25\n",
        "        reward = (w1 * qed) + (w2 * (1 - abs(logp - 2.5) / 2.5)) - (w3 * sa_penalty)\n",
        "        return max(0, reward) # Ensure reward is non-negative\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "# Cell 11: RL Fine-tuning Loop (Efficient Batched Version)\n",
        "print(\"\\nStarting RL Fine-tuning (Batched)...\")\n",
        "optimizer_g_rl = optim.Adam(generator.parameters(), lr=LR / 10)\n",
        "\n",
        "N_RL_STEPS = 100\n",
        "for step in range(N_RL_STEPS):\n",
        "    generator.train()\n",
        "    optimizer_g_rl.zero_grad()\n",
        "\n",
        "    # --- EFFICIENT BATCHED GENERATION ---\n",
        "    # Store log probabilities and generated selfies for each item in the batch\n",
        "    batch_log_probs = [[] for _ in range(BATCH_SIZE)]\n",
        "    generated_selfies = [\"\" for _ in range(BATCH_SIZE)]\n",
        "\n",
        "    # Keep track of which sequences in the batch are still being generated\n",
        "    active_mask = [True] * BATCH_SIZE\n",
        "\n",
        "    # Start with a batch of <sos> tokens\n",
        "    input_seq = torch.full((BATCH_SIZE, 1), token_to_int['<sos>'], device=device, dtype=torch.long)\n",
        "\n",
        "    # Autoregressively generate for MAX_LEN steps\n",
        "    for t in range(MAX_LEN - 1):\n",
        "        if not any(active_mask): # Stop if all sequences in the batch are finished\n",
        "            break\n",
        "\n",
        "        output_logits = generator(input_seq, input_seq)\n",
        "        probs = F.softmax(output_logits[:, -1, :], dim=-1)\n",
        "        next_token_ids = torch.multinomial(probs, 1)\n",
        "\n",
        "        # Update sequences for all active items in the batch\n",
        "        for i in range(BATCH_SIZE):\n",
        "            if active_mask[i]:\n",
        "                token_id = next_token_ids[i].item()\n",
        "\n",
        "                # If <eos> is generated, mark the sequence as finished\n",
        "                if token_id == token_to_int['<eos>']:\n",
        "                    active_mask[i] = False\n",
        "                else:\n",
        "                    # Append log probability for the chosen token\n",
        "                    log_prob = torch.log(probs[i, token_id])\n",
        "                    batch_log_probs[i].append(log_prob)\n",
        "\n",
        "                    # Append the token to the generated SELFIES string\n",
        "                    generated_selfies[i] += int_to_token[token_id]\n",
        "\n",
        "        # Append the new tokens to the input sequence for the next step\n",
        "        input_seq = torch.cat([input_seq, next_token_ids], dim=1)\n",
        "\n",
        "    # --- Calculate rewards and policy loss for the entire batch ---\n",
        "    batch_rewards = [calculate_reward(s) for s in generated_selfies]\n",
        "\n",
        "    policy_loss = 0\n",
        "    for i in range(BATCH_SIZE):\n",
        "        # Only calculate loss if the sequence is valid (has log probs)\n",
        "        if batch_log_probs[i]:\n",
        "            # The REINFORCE algorithm update rule\n",
        "            policy_loss += -torch.stack(batch_log_probs[i]).sum() * batch_rewards[i]\n",
        "\n",
        "    # Average the loss over the batch and perform backpropagation\n",
        "    if BATCH_SIZE > 0:\n",
        "        policy_loss /= BATCH_SIZE\n",
        "        policy_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
        "        optimizer_g_rl.step()\n",
        "\n",
        "    if (step + 1) % 10 == 0:\n",
        "        print(f\"[RL Step {step+1}/{N_RL_STEPS}] [Avg Reward: {np.mean(batch_rewards):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcWJq-jH5qs7"
      },
      "outputs": [],
      "source": [
        "# Cell 12: Generation and Evaluation Function\n",
        "def generate_molecules(generator, num_molecules=5):\n",
        "    \"\"\"Generates molecules from the trained generator and evaluates them.\"\"\"\n",
        "    generator.eval()\n",
        "    print(\"\\n--- Generating Final Molecules ---\")\n",
        "    for i in range(num_molecules):\n",
        "        with torch.no_grad():\n",
        "            input_seq = torch.tensor([[token_to_int['<sos>']]], device=device)\n",
        "            generated_selfies = \"\"\n",
        "            for _ in range(MAX_LEN - 1):\n",
        "                output_logits = generator(input_seq, input_seq)\n",
        "                next_token_id = torch.argmax(output_logits[:, -1, :], dim=-1).unsqueeze(0)\n",
        "\n",
        "                if next_token_id.item() == token_to_int['<eos>']: break\n",
        "\n",
        "                token = int_to_token[next_token_id.item()]\n",
        "                generated_selfies += token\n",
        "                input_seq = torch.cat([input_seq, next_token_id], dim=1)\n",
        "\n",
        "        smi = sf.decoder(generated_selfies)\n",
        "        reward = calculate_reward(generated_selfies)\n",
        "        print(f\"Molecule {i+1}: {smi} | Reward: {reward:.3f}\")\n",
        "\n",
        "# Generate some molecules from the final model\n",
        "generate_molecules(generator, num_molecules=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNkU-B6J7i_B"
      },
      "source": [
        "#STOP HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPvJ0hQ-R8X4"
      },
      "outputs": [],
      "source": [
        "# --- In your Colab Notebook ---\n",
        "\n",
        "import json\n",
        "\n",
        "# 1. Save the model's learned weights (the state_dict)\n",
        "torch.save(generator.state_dict(), 'generator_model.pth')\n",
        "\n",
        "# 2. Save the tokenizer vocabulary\n",
        "tokenizer_data = {\n",
        "    'token_to_int': token_to_int,\n",
        "    'int_to_token': {int(k): v for k, v in int_to_token.items()} # Ensure keys are JSON compatible\n",
        "}\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "    json.dump(tokenizer_data, f)\n",
        "\n",
        "# 3. Download the files\n",
        "from google.colab import files\n",
        "files.download('generator_model.pth')\n",
        "files.download('tokenizer.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GscBaOlSXfgV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18546673187c4cad9470f0f73436557f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "308d5df0f6c5406b9409350f4f21a647": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47e47b9ee2114467b16a5a3598549503",
            "placeholder": "​",
            "style": "IPY_MODEL_4b48b49b49204787aee6aeeb41e2a0c4",
            "value": " 1936962/1936962 [09:14&lt;00:00, 2463.13it/s]"
          }
        },
        "32274995ef8f413296d9be0a1c3da1b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47e47b9ee2114467b16a5a3598549503": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b48b49b49204787aee6aeeb41e2a0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b946c4ec47942ce88ac9e6676595aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6becc4fc0f244881887d3a6de1cc3cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d50bc005c7d2401a949f7dfc504986b6",
              "IPY_MODEL_98951c648f9d40838666155b358738a1",
              "IPY_MODEL_308d5df0f6c5406b9409350f4f21a647"
            ],
            "layout": "IPY_MODEL_32274995ef8f413296d9be0a1c3da1b0"
          }
        },
        "6e12a98dd24d448cbbc47ffb919fdd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98951c648f9d40838666155b358738a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18546673187c4cad9470f0f73436557f",
            "max": 1936962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b946c4ec47942ce88ac9e6676595aa3",
            "value": 1936962
          }
        },
        "d50bc005c7d2401a949f7dfc504986b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd59e1082a034203857a02615e23f6e5",
            "placeholder": "​",
            "style": "IPY_MODEL_6e12a98dd24d448cbbc47ffb919fdd21",
            "value": "Encoding SELFIES: 100%"
          }
        },
        "fd59e1082a034203857a02615e23f6e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}